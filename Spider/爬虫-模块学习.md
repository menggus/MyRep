## 爬虫-模块学习

### request模块（模拟浏览器进行请求）

### retry模块--（重复执行请求）

### 数据提取（提取响应中的数据，存储至数据库）

#### 数据分类： 非结构化数据  &  结构化数据

```python
# 非结构化数据：	
    html
    处理方法：正则表达式、xpath
    
# 结构化数据：
    json	xml
    处理方法：python转换
```
#### JSON（Javascript Object Notation，轻量级数据交换/交互格式）数据处理：

![1554959718211](C:\Users\tao_cp\AppData\Roaming\Typora\typora-user-images\1554959718211.png)

######							      Tips: 类文件对象--- with  open()  as  f:   其中 f   就是 类文件对象

```python
# 获取json数据 <chrome切换至手机版网页进行查看>；
import json
# json.dumps(dict)  # 把python类型转换为json格式
# json.loads(data1)  # 把json数据转换为python中字典格式

# 把dict以json格式存在本地
dict = {"a":1, "b":2, "c":3}
with open("dict.json", "w", encoding="utf-8") as f:
    """
    ensure_ascii: 设置不以ascii码进行显示
    indent: 设置写入文件的缩进 4空格 
    """
    data = json.dumps(dict, ensure_ascii=False, indent=4)  
    f.write(data)
# 从本地读取json文件
with open("dict.json", "r") as f:
    data = f.read()
    print(data)
    print(type(data))  # 字符串<class 'str'>
    data1 = json.loads(data)
    print(data1)
    print(type(data1))  # 字典<class 'dict'>
```

####  JSON使用注意：

- JSON字符串均使用   < 双引号>
- 如果不是双引号，怎么处理
  - 使用eval：能实现简单的字符串和python类型的转化
  - replace：把类型中的 <单引号> 替换为 <双引号> 

#### 正则表达式

```python
>>> import re
# findall返回小括号之间的内容(a bc起到筛选作用)，r.S可以匹配\n
>>> re.findall(r"a(.*?)bc", "asdcbc", r.S)  
['sdc']
# 原始字符串 r  ，当匹配字段中有反斜杠存在时使用，可以忽视反斜杠的转义
>>> re.findall("a\nbc", "a\nbc")
['a\nbc']
>>> re.findall("a\\nbc", "a\\nbc")  # 匹配不到
[]
>>> re.findall(r"a\\nbc", "a\\nbc")  # 加上r之后能匹配到
['a\\nbc']

# . 号默认情况是匹配不到 \n 的，需要在匹配规则后加上  r.S || r.DOTALL

# \s 可以匹配到  空白字符（\t \n \r）等
```



### Xpath：

注意：练习中使用工具xpath是基于elements，往往不能用于爬虫中，因为爬虫中是基于响应体的，即原始html和爬虫爬取的html不一样。

![1554994141674](C:\Users\tao_cp\AppData\Roaming\Typora\typora-user-images\1554994141674.png)

```xpath
# 获取文本
a/text()  # 获取a标签下文本
a//text()  # 获取a标签下的所有文本
//a[text()='下一页']  # 获取包含文本为'下一页'的a标签

@ 符号
a/@herf   # 获取a标签的herf属性
//ul[@id="detail-list"]  # //直接定位页面所有ul，[@id="detail-list"]对ul进行精确定位，条件为id=“detail-list”

// 符号
- 开头使用表示为从html页面定位
- li//a 表示从li标签下开始定位
```

![1554994957154](C:\Users\tao_cp\AppData\Roaming\Typora\typora-user-images\1554994957154.png)

![1554995133575](C:\Users\tao_cp\AppData\Roaming\Typora\typora-user-images\1554995133575.png)

```
//div[@id="page"]/a[1]   # 表示选择第一个a标签
/a[2]  # 表示第二个a标签
/a[last()]  # 选择最后一个a标签
/a[position()<4]  # 选择前3个a标签
/a[]position()>4]  # 表示选择第5个之后的a标签
```



### LXML 模块

```python
# lxml模块可以修正html代码，但可能存在bug：
html = etree.HTML(text)  # text：bytes类型或者string类型

# 使用etree.tostring()观察修改之后的html代码，根据修改后的html代码来写xpath。
print(etree.tostring(html).decode())  # 可查看修正后的代码

# 通过xpath获取属性和文本
result = html.xpath("")  # ""之间直接写xpath表达式 
```

```python
# lxml模块-提取页面数据
# 1. 先对element对象，进行数据分组，取到包含分组标签的列表
textdata = html.xpath("//li[@class='item-1']")
# 2. 遍历列表，再对每一组进行数据的提取
dict = []
for i in textdata:
    item = {}
    # item["title"] = i.xpath("./a/text()")[0] 取index=0时，可能报错，因为可能i.xpath()为空列表
    # 解决方法：使用三元运算符,如下：
    item["title"] = i.xpath("./a/text()")[0] if len(i.xpath("./a/text()")) > 0 else None
    item["link"] = i.xpath("./a/@href")[0] if len(i.xpath("./a/@href")) > 0 else None
    dict.append(item)
print(dict)
```



### 爬虫思路总结：

####  一、URL

```python
# 准备 start_url:
# 1.不确定爬取页面的url的数量；不确定url的规律
# 2.通过xpath在页面中获取下一页面的url地址
# 3.寻找url地址，查看url地址参数的变化（如单页面的爬取数据总数，显示列表的总数）

# 准备 url list:
# 1.确定爬取页面的url的数量	
# 2.知道url的变换规律
```

#### 二、发送请求，获取响应（伪装）

```python
# 1.添加随机的User-Agent，反反爬虫
# 2.添加随机的代理ip，反反爬虫
# 3.在对方判断出我们是爬虫之后，应该添加更多的headers字段，包括cookie
# 4.cookie的处理可以使用session来解决
# 5.准备一堆能用的cookie，组成cookie池
	# a.如果不登录
		# 准备刚开始能够成功请求对方网站的cookie，即接收对方网站设置在response的cookie
		# 下一次请求的时候，使用之前的列表中的cookie来请求
	# b.如果登陆
		# 准备多个账号
		# 使用程序获取每个账号的cookie
		# 然后请求登录之后才能访问的网站随机cookie
```



#### 三、提取数据

```python
# 1.确定数据的位置
	# 如果数据在当前的url地址页面中
		# 提取的是列表页的数据
			# 直接请求列表页的url地址，不用进入详情页
		# 提取的是详情页的数据（递归）
			# 1.确定url
			# 2.发送请求
			# 3.提取详情页数据
			# 4.返回
	# 如果提取的数据不再当前url地址的页面中
		# 在其他的响应中，寻找数据的位置
			# 1.从network中从上往下找
			# 2.使用chrome中的过滤条件，选择除了js，css，img之外的按钮
			# 3.使用chrome的search all file，搜索数字和英文
# 2.数据的提取
	# xpath 从html中提取整块的数据，先分组，之后遍历每一组进行提取
	# re  提取max_time,price,html中json字符串
	# json
```

#### 四、数据保存

```python
# 1.保存在本地： txt，json，csv等
# 2.保存在数据库中
```




